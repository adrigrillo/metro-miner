{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @metro_madrid tweets analysis\n",
    "\n",
    "This notebook will be focused in the analysis of the official metro account tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load of the libraries and set of globals variables, in this case, the id of the account (@metro_madrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_texts = config['keywords']['faults'].split(',')\n",
    "metro_account_id = config['accounts']['metro_madrid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tweets in list of the tweet object used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fault_tweets.json') as json_data:\n",
    "    faults = json.load(json_data)\n",
    "with open('../data/solution_tweets.json') as json_data:\n",
    "    solutions = json.load(json_data)\n",
    "with open('../data/strikes_tweets.json') as json_data:\n",
    "    strikes = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Circulación interrumpida en L6, entre Laguna y Moncloa en ambos sentidos, por causas técnicas. Tiempo estimado más de 15 minutos.', 'date': '2018-04-05T10:08:34', 'user_id': 182764833, 'place': None, 'country': None, 'country_code': None}\n",
      "['servicio interrumpido', 'circulación interrumpida', 'circulación lenta', 'los trenes no efectúan parada', 'tramo interrumpido', '']\n"
     ]
    }
   ],
   "source": [
    "print(faults[0])\n",
    "print(faults_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition\n",
    "We can create recognizers that will use a matcher to tag the tokens, docs and spans that contains a determinate pattern.\n",
    "This will be useful to clasify lines, station, problems and estimated waiting times in the tweets that the metro account\n",
    "post in twiter.\n",
    "\n",
    "We are going to create a metro line recognizer that will identify the line that has the fault. To do so, we will have to configure the the matcher indicating the lines of the metro line. We will use the next list that can be found in `config.ini` file:\n",
    "\n",
    "```data\n",
    "línea 1,línea 2,línea 3,línea 4,línea 5,línea 6,línea 7,línea 8,línea 9,línea 10,línea 11,línea 12,L1,L2,L3,L4,L5,L6,L7,L8,L9,L10,L11,L12\n",
    "\n",
    "```\n",
    "\n",
    "Generally, the official metro account will use the string `LX` (where x is the number of the line) but as we want to use tweets from other accounts, the might use the complete name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroLinesRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system lines\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the lines.\n",
    "    \n",
    "    The lines are labelled as FACILITY and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_line and ._.is_metro_line is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'metro_lines'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['FACILITY']  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        metro_lines = config['keywords']['lines'].split(',')\n",
    "        patterns = [nlp(org) for org in metro_lines]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_LINES', None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_line', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_line', getter=self.has_metro_line)\n",
    "        Span.set_extension('has_metro_line', getter=self.has_metro_line)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_line', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_line(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_line') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroLinesRecognizer2(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system lines\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the lines.\n",
    "    \n",
    "    The lines are labelled as FACILITY and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_line and ._.is_metro_line is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'metro_lines_token'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['FACILITY']  # get entity label ID\n",
    "\n",
    "        line_flag = lambda text: bool(re.compile('l([i|í]nea){0,1}', re.IGNORECASE).match(text))\n",
    "        IS_LINE = nlp.vocab.add_flag(line_flag)\n",
    "        metro_line_flag = lambda text: bool(re.compile('l([i|í]nea){0,1}[ -]{0,1}(1[0-2]|[1-9])', re.IGNORECASE).match(text))\n",
    "        IS_METRO_LINE = nlp.vocab.add_flag(metro_line_flag)\n",
    "        short_line_flag = lambda text: bool(re.compile('l[ -]{0,1}(1[0-2]|[1-9])', re.IGNORECASE).match(text))\n",
    "        IS_LINE_SHORT = nlp.vocab.add_flag(metro_line_flag)\n",
    "        \n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_LINES', None, [{IS_LINE: True}, {'IS_SPACE': True}, {'IS_DIGIT': True}], \n",
    "                         [{IS_LINE: True}, {'IS_DIGIT': True}], [{IS_METRO_LINE: True}], [{IS_LINE_SHORT: True}])\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_line', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_line', getter=self.has_metro_line)\n",
    "        Span.set_extension('has_metro_line', getter=self.has_metro_line)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_line', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_line(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_line') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the lines recognizer to the pipeline to assure it is executed in the pre-processing of the text and save us the step of executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_detector = MetroLinesRecognizer(nlp=nlp)\n",
    "nlp.add_pipe(lines_detector, last=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'metro_lines_token']\n"
     ]
    }
   ],
   "source": [
    "lines_detector = MetroLinesRecognizer2(nlp=nlp)\n",
    "nlp.add_pipe(lines_detector, last=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try it with a tweet containing the official account style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.remove_pipe('metro_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_about_lines(text):\n",
    "    doc = nlp(text)\n",
    "    doc.ents\n",
    "    print('The text contains a metro line in it: ', doc._.has_metro_line)\n",
    "    print('Tokens and if it is part of a line:', [(t, t._.is_metro_line) for t in doc])\n",
    "    print('Tokens that form part of a line:', [t for t in doc if t._.is_metro_line])\n",
    "    print('Entities that are a facility:', [(e.text, e.label_) for e in doc.ents if e.label_ == 'FACILITY']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación interrumpida en L6, entre Laguna y Moncloa en ambos sentidos, por causas técnicas. Tiempo estimado más de 15 minutos.\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(Circulación, False), (interrumpida, False), (en, False), (L6, True), (,, False), (entre, False), (Laguna, False), (y, False), (Moncloa, False), (en, False), (ambos, False), (sentidos, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False), (Tiempo, False), (estimado, False), (más, False), (de, False), (15, False), (minutos, False), (., False)]\n",
      "Tokens that form part of a line: [L6]\n",
      "Entities that are a facility: [('L6', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[0]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, when the text contains and station the value of the attribute `has_metro_line` is `True`. Also the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en línea 6 entre Príncipe Pío y Metropolitano, dirección andén 2, por causas técnicas.\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(Circulación, False), (lenta, False), (en, False), (línea 6, True), (entre, False), (Príncipe, False), (Pío, False), (y, False), (Metropolitano, False), (,, False), (dirección, False), (andén, False), (2, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False)]\n",
      "Tokens that form part of a line: [línea 6]\n",
      "Entities that are a facility: [('línea 6', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[1]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación interrumpida en LÍNEA 10 en ambos sentidos entre Nuevos Ministerios y Begoña por asistencia sanitaria a un viajero. Tiempo estimado de solución +30 min\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(Circulación, False), (interrumpida, False), (en, False), (LÍNEA 10, True), (en, False), (ambos, False), (sentidos, False), (entre, False), (Nuevos, False), (Ministerios, False), (y, False), (Begoña, False), (por, False), (asistencia, False), (sanitaria, False), (a, False), (un, False), (viajero, False), (., False), (Tiempo, False), (estimado, False), (de, False), (solución, False), (+, False), (30, False), (min, False)]\n",
      "Tokens that form part of a line: [LÍNEA 10]\n",
      "Entities that are a facility: [('LÍNEA 10', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[2]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tramo de circulación interrumpida en linea10 por asistencia sanitaria a un viajero pasa a ser entre Begoña y Cuzco. Tiempo estimado solución + 30 min.\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(El, False), (tramo, False), (de, False), (circulación, False), (interrumpida, False), (en, False), (linea10, True), (por, False), (asistencia, False), (sanitaria, False), (a, False), (un, False), (viajero, False), (pasa, False), (a, False), (ser, False), (entre, False), (Begoña, False), (y, False), (Cuzco, False), (., False), (Tiempo, False), (estimado, False), (solución, False), (+, False), (30, False), (min, False), (., False)]\n",
      "Tokens that form part of a line: [linea10]\n",
      "Entities that are a facility: [('linea10', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[3]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en Linea 1 entre Bilbao y Pinar de Chamartín, en ambos sentidos, por causas técnicas.\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(Circulación, False), (lenta, False), (en, False), (Linea 1, True), (entre, False), (Bilbao, False), (y, False), (Pinar, False), (de, False), (Chamartín, False), (,, False), (en, False), (ambos, False), (sentidos, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False)]\n",
      "Tokens that form part of a line: [Linea 1]\n",
      "Entities that are a facility: [('Linea 1', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[4]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en Línea 2 entre las estaciones de Goya y Avenida de Guadalajara, en ambos sentidos por causas técnicas. Tiempo estimado de solución: +30 min\n",
      "The text contains a metro line in it:  True\n",
      "Tokens and if it is part of a line: [(Circulación, False), (lenta, False), (en, False), (Línea 2, True), (entre, False), (las, False), (estaciones, False), (de, False), (Goya, False), (y, False), (Avenida, False), (de, False), (Guadalajara, False), (,, False), (en, False), (ambos, False), (sentidos, False), (por, False), (causas, False), (técnicas, False), (., False), (Tiempo, False), (estimado, False), (de, False), (solución, False), (:, False), (+, False), (30, False), (min, False)]\n",
      "Tokens that form part of a line: [Línea 2]\n",
      "Entities that are a facility: [('Línea 2', 'FACILITY')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[5]['text']\n",
    "print(test)\n",
    "print_data_about_lines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN = re.compile('l([i|í]nea){0,1}[ -]{0,1}(1[0-2]|[1-9])', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bool(PATTERN.match(\"linea 1\")))\n",
    "print(bool(PATTERN.match(\"linea10\")))\n",
    "print(bool(PATTERN.match(\"línea 5\")))\n",
    "print(bool(PATTERN.match(\"LÍNEA 22\")))\n",
    "print(bool(PATTERN.match(\"L 10\")))\n",
    "print(bool(PATTERN.match(\"L-1\")))\n",
    "print(bool(PATTERN.match(\"Las\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
