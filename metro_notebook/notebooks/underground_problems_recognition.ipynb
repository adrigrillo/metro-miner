{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Madrid underground problems recognition\n",
    "\n",
    "This notebook will be focused in the recognition of the different faults that could happen in the Madrid underground system. The official account informs about three kind of situations that can affect the circulation, these are:\n",
    "\n",
    "- **Faults:** when are produced and where they are fixed.\n",
    "- **Delays:** when they start and where the circulation is normalized.\n",
    "- **Strikes:** reminding the days that there is a strike called.\n",
    "\n",
    "Some examples of the texts that form the tweets indicating this alterations are:\n",
    "\n",
    "- **Faults:** servicio interrumpido, circulación interrumpida, circulación lenta, trenes no efectúan parada, tramo interrumpido.\n",
    "- **Delays:** retrasos, min de espera, minutos de espera, minutos esperando.\n",
    "- **Strikes:** huelga, servicios mínimos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_texts = config['keywords']['faults'].split(',')\n",
    "metro_account_id = config['accounts']['metro_madrid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fault_tweets.json') as json_data:\n",
    "    faults = json.load(json_data)\n",
    "with open('../data/solution_tweets.json') as json_data:\n",
    "    solutions = json.load(json_data)\n",
    "with open('../data/strikes_tweets.json') as json_data:\n",
    "    strikes = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroFaultsRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system fautls\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the faults.\n",
    "    \n",
    "    The faults are labelled as an EVENT and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_fault and ._.is_metro_fault is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'faults_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_FAULTS', None, [{'LOWER': 'circulación'}, {'LOWER': 'interrumpida'}],\n",
    "                         [{'LOWER': 'incidencia'}],\n",
    "                         [{'LOWER': 'circulacion'}, {'LOWER': 'interrumpida'}],\n",
    "                         [{'LOWER': 'circulación'}, {'LOWER': 'lenta'}],\n",
    "                         [{'LOWER': 'circulacion'}, {'LOWER': 'lenta'}],\n",
    "                         [{'LOWER': 'tramo'}, {'LOWER': 'interrumpido'}],\n",
    "                         [{'LOWER': 'tramo'}, {'LOWER': 'cortado'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'interrumpido'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'cortado'}],\n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectúan'}, {'LOWER': 'parada'}], \n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectuan'}, {'LOWER': 'parada'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_fault', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_fault', getter=self.has_metro_fault)\n",
    "        Span.set_extension('has_metro_fault', getter=self.has_metro_fault)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_fault', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_fault(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_fault') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroDelaysRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system delays\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the delays.\n",
    "    \n",
    "    The delays are labelled as an EVENT and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_delay and ._.is_metro_delay is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'delays_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_DELAYS', None, [{'LOWER': 'retrasos'}], [{'LOWER': 'retraso'}], [{'LOWER': 'frequencia'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'min'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'de'}, {'LOWER': 'retraso'}],\n",
    "                         [{'LOWER': 'min'}, {'LOWER': 'de'}, {'LOWER': 'retraso'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'esperando'}],\n",
    "                         [{'LOWER': 'tiempo'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'tiempos'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'frecuencia'}, {'LOWER': 'de'}, {'LOWER': 'paso'}],\n",
    "                         [{'LOWER': 'frecuencias'}, {'LOWER': 'de'}, {'LOWER': 'paso'}],\n",
    "                         [{'LOWER': 'frecuencias'}, {'LOWER': 'de'}, {'LOWER': 'trenes'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_delay', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_delay', getter=self.has_metro_delay)\n",
    "        Span.set_extension('has_metro_delay', getter=self.has_metro_delay)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_delay', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_delay(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_delay') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroSolutionsRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system solutions to faults\n",
    "    abd delays and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the solutions.\n",
    "    \n",
    "    The solutions are labelled as an EVENT and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_solution and ._.is_metro_solution is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'solutions_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_SOLUTIONS', None, [{'LOWER': 'circulación'}, {'LOWER': 'normalizada'}],\n",
    "                         [{'LOWER': 'circulacion'}, {'LOWER': 'normalizada'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'normalizado'}],\n",
    "                         [{'LOWER': 'normalizado'}, {'LOWER': 'el'}, {'LOWER': 'servicio'}],\n",
    "                         [{'LOWER': 'restablecido'}, {'LOWER': 'el'}, {'LOWER': 'servicio'}],\n",
    "                         [{'LOWER': 'ya'}, {'LOWER': 'efectúan'}, {'LOWER': 'parada'}],\n",
    "                         [{'LOWER': 'ya'}, {'LOWER': 'efectuan'}, {'LOWER': 'parada'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_solution', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_solution', getter=self.has_metro_solution)\n",
    "        Span.set_extension('has_metro_solution', getter=self.has_metro_solution)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_solution', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_solution(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_solution') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroStrikesRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system strikes\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the delays.\n",
    "    \n",
    "    The strikes are labelled as an EVENT and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_strike and ._.is_metro_strike is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'strikes_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_STRIKES', None, [{'LOWER': 'huelga'}], [{'LOWER': 'paros'}],\n",
    "                         [{'LOWER': 'servicios'}, {'LOWER': 'minimos'}],\n",
    "                         [{'LOWER': 'servicios'}, {'LOWER': 'mínimos'}],\n",
    "                         [{'LOWER': 'paros'}, {'LOWER': 'convocados'}, {'LOWER': 'para'}, {'LOWER': 'hoy'}],\n",
    "                         [{'LOWER': 'paros'}, {'LOWER': 'convocados'}, {'LOWER': 'para'}, {'LOWER': 'mañana'}],\n",
    "                         [{'LOWER': 'paros'}, {'LOWER': 'convocados'}, {'LOWER': 'para'}, {'LOWER': 'el'}, \n",
    "                          {'LOWER': 'dia'}, {'IS_DIGIT': True}],\n",
    "                         [{'LOWER': 'paros'}, {'LOWER': 'convocados'}, {'LOWER': 'para'}, {'LOWER': 'el'}, \n",
    "                          {'LOWER': 'día'}, {'IS_DIGIT': True}],\n",
    "                         [{'LOWER': 'paros'}, {'LOWER': 'convocados'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_strike', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_strike', getter=self.has_metro_strike)\n",
    "        Span.set_extension('has_metro_strike', getter=self.has_metro_strike)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_strike', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_strike(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_strike') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'faults_recognizer', 'delays_recognizer', 'solutions_recognizer', 'strikes_recognizer']\n"
     ]
    }
   ],
   "source": [
    "fault_detector = MetroFaultsRecognizer(nlp=nlp)\n",
    "delay_detector = MetroDelaysRecognizer(nlp=nlp)\n",
    "solution_detector = MetroSolutionsRecognizer(nlp=nlp)\n",
    "strike_detector = MetroStrikesRecognizer(nlp=nlp)\n",
    "nlp.add_pipe(fault_detector, last=True)\n",
    "nlp.add_pipe(delay_detector, last=True)\n",
    "nlp.add_pipe(solution_detector, last=True)\n",
    "nlp.add_pipe(strike_detector, last=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try it with a tweet containing the official account style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recognizer_info(text):\n",
    "    doc = nlp(text)\n",
    "    doc.ents\n",
    "    print('The text contains a fault in it: ', doc._.has_metro_fault)\n",
    "    print('The text contains a delay in it: ', doc._.has_metro_delay)\n",
    "    print('The text contains a solution in it: ', doc._.has_metro_solution)\n",
    "    print('The text contains a strike in it: ', doc._.has_metro_strike)\n",
    "    print('Entities that are an event:', [(e.text, e.label_) for e in doc.ents if e.label_ == 'EVENT']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación interrumpida en L6, entre Laguna y Moncloa en ambos sentidos, por causas técnicas. Tiempo estimado más de 15 minutos.\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulación interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[0]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en línea 6 entre Príncipe Pío y Metropolitano, dirección andén 2, por causas técnicas.\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulación lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[1]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulacion interrumpida en LÍNEA 10 en ambos sentidos entre Nuevos Ministerios y Begoña por asistencia sanitaria a un viajero. Tiempo estimado de solución +30 min\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulacion interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[2]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tramo de circulación interrumpida en linea10 por asistencia sanitaria a un viajero pasa a ser entre Begoña y Cuzco. Tiempo estimado solución + 30 min.\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('circulación interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[3]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulacion lenta en Linea 1 entre Bilbao y Pinar de Chamartín, en ambos sentidos, por causas técnicas.\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulacion lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[4]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en Línea 2 entre las estaciones de Goya y Avenida de Guadalajara, en ambos sentidos por causas técnicas. Tiempo estimado de solución: +30 min\n",
      "The text contains a fault in it:  True\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulación lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[5]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación normalizada en L6 entre Laguna y Moncloa.\n",
      "The text contains a fault in it:  False\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  True\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Circulación normalizada', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = solutions[0]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restablecido el servicio en L6 entre Metropolitano y Argüelles.\n",
      "The text contains a fault in it:  False\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  True\n",
      "The text contains a strike in it:  False\n",
      "Entities that are an event: [('Restablecido el servicio', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = solutions[5]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ RECUERDA: servicios mínimos durante los paros convocados para hoy:\n",
      "The text contains a fault in it:  False\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  True\n",
      "Entities that are an event: [('servicios mínimos', 'EVENT'), ('paros convocados para hoy', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = strikes[0]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡ INFORMACIÓN: servicios mínimos durante los paros convocados para el día 15 de marzo:\n",
      "The text contains a fault in it:  False\n",
      "The text contains a delay in it:  False\n",
      "The text contains a solution in it:  False\n",
      "The text contains a strike in it:  True\n",
      "Entities that are an event: [('servicios mínimos', 'EVENT'), ('paros convocados para el día 15', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = strikes[1]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
