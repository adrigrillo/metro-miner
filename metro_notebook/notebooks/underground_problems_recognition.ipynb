{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Madrid underground problems recognition\n",
    "\n",
    "This notebook will be focused in the recognition of the different faults that could happen in the Madrid underground system. The official account informs about three kind of situations that can affect the circulation, these are:\n",
    "\n",
    "- **Faults:** when are produced and where they are fixed.\n",
    "- **Delays:** when they start and where the circulation is normalized.\n",
    "- **Strikes:** reminding the days that there is a strike called.\n",
    "\n",
    "Some examples of the texts that form the tweets indicating this alterations are:\n",
    "\n",
    "- **Faults:** servicio interrumpido, circulación interrumpida, circulación lenta, trenes no efectúan parada, tramo interrumpido.\n",
    "- **Delays:** retrasos, min de espera, minutos de espera, minutos esperando.\n",
    "- **Strikes:** huelga, servicios mínimos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faults_texts = config['keywords']['faults'].split(',')\n",
    "metro_account_id = config['accounts']['metro_madrid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fault_tweets.json') as json_data:\n",
    "    faults = json.load(json_data)\n",
    "with open('../data/solution_tweets.json') as json_data:\n",
    "    solutions = json.load(json_data)\n",
    "with open('../data/strikes_tweets.json') as json_data:\n",
    "    strikes = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroFaultsRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system lines\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the lines.\n",
    "    \n",
    "    The lines are labelled as FACILITY and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_line and ._.is_metro_line is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'faults_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        \n",
    "        #LINE_PATTERN = re.compile('l([i|í]nea){0,1}', re.IGNORECASE)\n",
    "        #LONG_METRO_LINE_PATTERN = re.compile('l([i|í]nea){0,1}[ -]{0,1}(1[0-2]|[1-9])', re.IGNORECASE)\n",
    "\n",
    "        #line_flag = lambda text: bool(LINE_PATTERN.match(text))\n",
    "        #IS_LINE = nlp.vocab.add_flag(line_flag)\n",
    "        #metro_line_flag = lambda text: bool(LONG_METRO_LINE_PATTERN.match(text))\n",
    "        #IS_METRO_LINE = nlp.vocab.add_flag(metro_line_flag)\n",
    "        \n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_LINES', None, [{'LOWER': 'circulación'}, {'LOWER': 'interrumpida'}],\n",
    "                         [{'LOWER': 'incidencia'}],\n",
    "                         [{'LOWER': 'circulacion'}, {'LOWER': 'interrumpida'}],\n",
    "                         [{'LOWER': 'circulación'}, {'LOWER': 'lenta'}],\n",
    "                         [{'LOWER': 'circulacion'}, {'LOWER': 'lenta'}],\n",
    "                         [{'LOWER': 'tramo'}, {'LOWER': 'interrumpido'}],\n",
    "                         [{'LOWER': 'tramo'}, {'LOWER': 'cortado'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'interrumpido'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'cortado'}],\n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectúan'}, {'LOWER': 'parada'}], \n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectuan'}, {'LOWER': 'parada'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_fault', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_fault', getter=self.has_metro_line)\n",
    "        Span.set_extension('has_metro_fault', getter=self.has_metro_line)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_fault', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_line(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_fault') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetroDelaysRecognizer(object):\n",
    "    \"\"\"Pipeline component that recognises the Madrid underground system lines\n",
    "    and sets entity annotations to the text that holds them. This allow the\n",
    "    easy recognition and handling of the lines.\n",
    "    \n",
    "    The lines are labelled as FACILITY and their spans are merged into one token.\n",
    "    Additionally, ._.has_metro_line and ._.is_metro_line is set on the \n",
    "    Doc/Span and Token respectively\n",
    "    \"\"\"\n",
    "    name = 'faults_recognizer'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp):\n",
    "        \"\"\"\n",
    "        Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the configured lines (config.ini) and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \n",
    "        :param nlp: spaCy nlp instance\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings['EVENT']  # get entity label ID\n",
    "        \n",
    "        #LINE_PATTERN = re.compile('l([i|í]nea){0,1}', re.IGNORECASE)\n",
    "        #LONG_METRO_LINE_PATTERN = re.compile('l([i|í]nea){0,1}[ -]{0,1}(1[0-2]|[1-9])', re.IGNORECASE)\n",
    "\n",
    "        #line_flag = lambda text: bool(LINE_PATTERN.match(text))\n",
    "        #IS_LINE = nlp.vocab.add_flag(line_flag)\n",
    "        #metro_line_flag = lambda text: bool(LONG_METRO_LINE_PATTERN.match(text))\n",
    "        #IS_METRO_LINE = nlp.vocab.add_flag(metro_line_flag)\n",
    "        \n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('METRO_LINES', None, [{'LOWER': 'retrasos'}], [{'LOWER': 'retraso'}], [{'LOWER': 'frequencia'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'min'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'de'}, {'LOWER': 'retraso'}],\n",
    "                         [{'LOWER': 'min'}, {'LOWER': 'de'}, {'LOWER': 'retraso'}],\n",
    "                         [{'LOWER': 'minutos'}, {'LOWER': 'esperando'}],\n",
    "                         [{'LOWER': 'tiempo'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'tiempos'}, {'LOWER': 'de'}, {'LOWER': 'espera'}],\n",
    "                         [{'LOWER': 'frecuencia'}, {'LOWER': 'de'}, {'LOWER': 'paso'}],\n",
    "                         [{'LOWER': 'frecuencias'}, {'LOWER': 'de'}, {'LOWER': 'paso'}],\n",
    "                         [{'LOWER': 'frecuencias'}, {'LOWER': 'de'}, {'LOWER': 'trenes'}],\n",
    "                         [{'LOWER': 'servicio'}, {'LOWER': 'cortado'}],\n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectúan'}, {'LOWER': 'parada'}], \n",
    "                         [{'LOWER': 'trenes'}, {'LOWER': 'no'}, {'LOWER': 'efectuan'}, {'LOWER': 'parada'}])\n",
    "        \n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_metro_fault', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_element_matched == True.\n",
    "        Doc.set_extension('has_metro_fault', getter=self.has_metro_line)\n",
    "        Span.set_extension('has_metro_fault', getter=self.has_metro_line)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \n",
    "        :param doc: text to be analysed\n",
    "        :return: text updated with the tags and the entities matched\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity and set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_metro_fault', True)\n",
    "            # Overwrite doc.ents and add entity\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token.\n",
    "            span.merge()\n",
    "        return doc\n",
    "\n",
    "    @staticmethod\n",
    "    def has_metro_line(tokens):\n",
    "        \"\"\"\n",
    "        Getter for Doc and Span attributes\n",
    "        \n",
    "        :param tokens: tokens of the Doc or the Span, that is, the text\n",
    "        :return: True if one of the tokens is a matched element\n",
    "        \"\"\"\n",
    "        return any([token._.get('is_metro_fault') for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'faults_recognizer']\n"
     ]
    }
   ],
   "source": [
    "fault_detector = MetroFaultsRecognizer(nlp=nlp)\n",
    "nlp.add_pipe(fault_detector, last=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try it with a tweet containing the official account style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recognizer_info(text):\n",
    "    doc = nlp(text)\n",
    "    doc.ents\n",
    "    print('The text contains a fault in the metro in it: ', doc._.has_metro_fault)\n",
    "    print('Tokens and if it is part of a fault:', [(t, t._.is_metro_fault) for t in doc])\n",
    "    print('Entities that are an event:', [(e.text, e.label_) for e in doc.ents if e.label_ == 'EVENT']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación interrumpida en L6, entre Laguna y Moncloa en ambos sentidos, por causas técnicas. Tiempo estimado más de 15 minutos.\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(Circulación interrumpida, True), (en, False), (L6, False), (,, False), (entre, False), (Laguna, False), (y, False), (Moncloa, False), (en, False), (ambos, False), (sentidos, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False), (Tiempo, False), (estimado, False), (más, False), (de, False), (15, False), (minutos, False), (., False)]\n",
      "Entities that are an event: [('Circulación interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[0]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en línea 6 entre Príncipe Pío y Metropolitano, dirección andén 2, por causas técnicas.\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(Circulación lenta, True), (en, False), (línea, False), (6, False), (entre, False), (Príncipe, False), (Pío, False), (y, False), (Metropolitano, False), (,, False), (dirección, False), (andén, False), (2, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False)]\n",
      "Entities that are an event: [('Circulación lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[1]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulacion interrumpida en LÍNEA 10 en ambos sentidos entre Nuevos Ministerios y Begoña por asistencia sanitaria a un viajero. Tiempo estimado de solución +30 min\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(Circulacion interrumpida, True), (en, False), (LÍNEA, False), (10, False), (en, False), (ambos, False), (sentidos, False), (entre, False), (Nuevos, False), (Ministerios, False), (y, False), (Begoña, False), (por, False), (asistencia, False), (sanitaria, False), (a, False), (un, False), (viajero, False), (., False), (Tiempo, False), (estimado, False), (de, False), (solución, False), (+, False), (30, False), (min, False)]\n",
      "Entities that are an event: [('Circulacion interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[2]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tramo de circulación interrumpida en linea10 por asistencia sanitaria a un viajero pasa a ser entre Begoña y Cuzco. Tiempo estimado solución + 30 min.\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(El, False), (tramo, False), (de, False), (circulación interrumpida, True), (en, False), (linea10, False), (por, False), (asistencia, False), (sanitaria, False), (a, False), (un, False), (viajero, False), (pasa, False), (a, False), (ser, False), (entre, False), (Begoña, False), (y, False), (Cuzco, False), (., False), (Tiempo, False), (estimado, False), (solución, False), (+, False), (30, False), (min, False), (., False)]\n",
      "Entities that are an event: [('circulación interrumpida', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[3]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulacion lenta en Linea 1 entre Bilbao y Pinar de Chamartín, en ambos sentidos, por causas técnicas.\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(Circulacion lenta, True), (en, False), (Linea, False), (1, False), (entre, False), (Bilbao, False), (y, False), (Pinar, False), (de, False), (Chamartín, False), (,, False), (en, False), (ambos, False), (sentidos, False), (,, False), (por, False), (causas, False), (técnicas, False), (., False)]\n",
      "Entities that are an event: [('Circulacion lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[4]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circulación lenta en Línea 2 entre las estaciones de Goya y Avenida de Guadalajara, en ambos sentidos por causas técnicas. Tiempo estimado de solución: +30 min\n",
      "The text contains a fault in the metro in it:  True\n",
      "Tokens and if it is part of a fault: [(Circulación lenta, True), (en, False), (Línea, False), (2, False), (entre, False), (las, False), (estaciones, False), (de, False), (Goya, False), (y, False), (Avenida, False), (de, False), (Guadalajara, False), (,, False), (en, False), (ambos, False), (sentidos, False), (por, False), (causas, False), (técnicas, False), (., False), (Tiempo, False), (estimado, False), (de, False), (solución, False), (:, False), (+, False), (30, False), (min, False)]\n",
      "Entities that are an event: [('Circulación lenta', 'EVENT')]\n"
     ]
    }
   ],
   "source": [
    "test = faults[5]['text']\n",
    "print(test)\n",
    "print_recognizer_info(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
